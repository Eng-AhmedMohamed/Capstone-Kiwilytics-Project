# Total Revenue per Day ETL with Airflow

This project builds an **ETL pipeline** using **Apache Airflow** to fetch data from a PostgreSQL database, transform it with Python (pandas), and calculate **total revenue per day**.  
The DAG automates the process on a daily schedule.

---

## Project Structure

```
total_revenue_per_day_airflow/
â”‚
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â””â”€â”€ total_revenue_dag.py   # Main DAG definition
â”œâ”€â”€ data/
â”‚    â””â”€â”€ categories.csv
â”‚    â””â”€â”€ customers.csv
â”‚    â””â”€â”€ employees.csv
â”‚    â””â”€â”€ order_details.csv
â”‚    â””â”€â”€ products.csv
â”‚    â””â”€â”€ sales.csv
â”‚    â””â”€â”€ shippers.csv
â”‚    â””â”€â”€ suppliers.csv
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ revenue_per_day_data.csv
â”‚    â””â”€â”€ sales_data.csv
â”‚   â””â”€â”€ total_revenue_per_day.png
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # Documentation
```

---

## Features

- **Extract**: Query order data from PostgreSQL
- **Transform**: Clean and aggregate revenue using pandas
- **Load**: Store transformed results back into local machine (or another destination)
- **Schedule**: Automated daily execution with Airflow

---

## Example DAG Flow

1. Extract data from Postgres
2. Transform using pandas (`groupby` on order_date, sum revenue)
3. Load results back into local machine

---

## Requirements

See [requirements.txt](./requirements.txt) for dependencies.

---

## ðŸ“Œ Future Improvements

- Integrate with Data Warehouse (Snowflake/BigQuery/Redshift)
